knitr::opts_chunk$set(echo = TRUE)
library(estimatr)
library(ggdag)
library(ggplot2)
library(readr)
library(tidyverse)
library(stargazer)
# Constructing a simple dag using dagify where x, y, a, b are the different nodes;
# x is the exposure and y is the outcome and we are trying to analyze the effect of x on y
simple_dag <- dagify( # specify relationships
y ~ x + a + b, # y depends on x, a and b
x ~ a + b, # x depends on a and b
exposure = "x", # "exposure" = treatment
outcome = "y"
)
# theme_dag() removes all axes and ticks, since those have little meaning in a
# causal model, and also makes a few other changes.
ggdag(simple_dag) + theme_dag()
# a and b are confounders
# Confounder example: Here, addictive behavior is a confounder that is causing
# coffee addition as well as smoking addiction which in turn is causing lung cancer.
# We are trying to analyze the effect of coffee on lung cancer.
coffee_dag <- dagify(cancer ~ smoking,
smoking ~ addictive, #Addictive behavior is causing smoking addiction
coffee ~ addictive, #Addictive behavior is causing coffee addiction
exposure = "coffee",
outcome = "cancer",
labels = c("coffee" = "Coffee", "cancer" = "Lung Cancer",
"smoking" = "Smoking", "addictive" = "Addictive \nBehavior")) %>%
tidy_dagitty(layout = "tree") #tidies the graph according to the layout
# labels with use_labels can be used to change the labels on the dags
ggdag(coffee_dag, text = FALSE, use_labels = "label")
# Objects of class tidy_dagitty or dagitty can be plotted quickly with ggdag().
# Collider : a node where two or more arrow heads meet. Stratifying on a collider is
# a major culprit in systematic bias. Controlling for a collider induces an association
# between its parents, through which confounding can flow.
# We assume that flu and chicken pox are unconfounded but both of them cause fevers.
fever_dag <- collider_triangle(x = "Influenza",
y = "Chicken Pox",
m = "Fever")
ggdag(fever_dag, text = FALSE, use_labels = "label")
# We don't need to control for fever as there is no effect on one by the other,
# nor are there any back-door paths:
# What happens if we control for fever?
# We open a biasing pathway between flu and chicken pox
ggdag_dseparated(fever_dag, controlling_for = "m",
text = FALSE, use_labels = "label")
# Whether or not you have a fever tells me something about your disease.
# If you have a fever, but you don’t have the flu, I now have more evidence that you have chicken pox.
# Constructing a simple dag to see the effects of smoking on cardiac arrest.
# We assume that Smoking causes cholesterol to rise,
# which then increases risk for cardiac arrest.
# Cholesterol is an intermediate variable between smoking and cardiac arrest.
smoking_ca_dag <- dagify(cardiacarrest ~ cholesterol,
cholesterol ~ smoking + weight,
smoking ~ unhealthy,
weight ~ unhealthy,
exposure = "smoking",
outcome = "cardiacarrest",
labels = c("cardiacarrest" = "Cardiac\n Arrest",
"smoking" = "Smoking",
"cholesterol" = "Cholesterol",
"unhealthy" = "Unhealthy\n Lifestyle",
"weight" = "Weight")
) %>% tidy_dagitty(layout = "tree")
ggdag_status(smoking_ca_dag, text = FALSE, use_labels = "label") + theme_dag()
# Do you see a confounder or a collider?
ggdag_paths(smoking_ca_dag, text = FALSE, use_labels = "label", shadow = TRUE)
# In addition to the directed pathway to cardiac arrest, there’s also an open
# back-door path through the forked path at unhealthy lifestyle and on from
# there through the chain to cardiac arrest. We need to account for this back-door
# path in our analysis.
# Accounting for weight will give us an unbiased estimate of the relationship
# between smoking and cardiac arrest, assuming our DAG is correct.
# We do not want to control for cholesterol because it’s an intermediate variable
# between smoking and cardiac arrest and controlling for it blocks the path
# between the two, which will then bias our estimate.
#reading the dataset
mosquito_nets <- read_csv("mosquito_nets.csv", show_col_types = FALSE)
#Constructing a simple dag with the mosquito_nets data using dagify
mosquito_dag <- dagify(
malaria_risk ~ net + income + health + temperature + resistance,
net ~ income + health + temperature + eligible + household,
eligible ~ income + household,
health ~ income,
exposure = "net",
outcome = "malaria_risk",
coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5,
temperature = 6, resistance = 8.5, eligible = 2, household = 1),
y = c(malaria_risk = 2, net = 2, income = 3, health = 1,
temperature = 3, resistance = 2, eligible = 3, household = 2)),
labels = c(malaria_risk = "Risk of malaria", net = "Mosquito net", income = "Income",
health = "Health", temperature = "Temperature",
resistance = "Resistance",
eligible = "Eligibility", household = "Household")
)
ggdag_status(mosquito_dag, use_labels = "label", text = FALSE) +
guides(fill = "none", color = "none") +  # Disable the legend
theme_dag()
# What are some of the relationships that you can see in this DAG?
#correlation between health and household
cor(mosquito_nets$health, mosquito_nets$household)
#correlation between resistance and household
cor(mosquito_nets$household, mosquito_nets$resistance)
#modelling malaria_risk using the household, health, income, net, temperature covariates
tidy(lm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets))
# What can be inferred from this result?
#using dagify to plot simple dag on labor market discrimination
earning_dag <- dagify(discrimination ~ female,
earning ~ discrimination + occupation + ability,
occupation ~ ability + discrimination,
exposure = "discrimination",
outcome = "earning",
labels = c("discrimination" = "discrimination",
"earning" = "earning",
"occupation" = "occupation",
"ability" = "ability",
"female" = "female"))
ggdag_status(earning_dag,
text = FALSE,
use_labels = "label") + theme_dag()
#generating a dataset
tb <- tibble(
#since female var is binary here we'll use uniform
female = ifelse(runif(10000)>=0.5,1,0),
#note that ability is drawn from standard normal and independent of female preferences
ability = rnorm(10000),
discrimination = female,
#occupations are increasing in unobserved ability but decreasing in discrimination.
occupation = 1 + 2*ability + 0*female - 2*discrimination + rnorm(10000),
#wages are decreasing in discrimination but increasing in higher-quality jobs and higher ability
wage = 1 - 1*discrimination + 1*occupation + 2*ability + rnorm(10000)
)
#run regressions adding one control at each step
lm_1 <- lm(wage ~ female, tb)
lm_2 <- lm(wage ~ female + occupation, tb)
lm_3 <- lm(wage ~ female + occupation + ability, tb)
stargazer(lm_1,lm_2,lm_3, type = "latex",
column.labels = c("Biased Unconditional",
"Biased",
"Unbiased Conditional"),
omit.stat=c("f","rsq","adj.rsq","ser"),
title = "Gender Discrimination")
police_dag <- dagify(Force ~ Controls,
Force ~ Stop,
Force ~ Minority,
Force ~ Suspicion,
Stop ~ Minority,
Stop ~ Suspicion,
Minority ~ Controls,
exposure = "Minority",
outcome = "Force",
labels = c("Force" = "Force",
"Stop" = "Stop",
"Minority" = "Minority",
"Controls"= "Controls",
"Suspicion" = "Suspicion"
))
ggdag_status(police_dag,
text = FALSE,
use_labels = "label") + theme_dag()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggdag)
library(broom)
library(haven)
library(ggpubr)
library(kableExtra)
set.seed(11)
rm(list = ls())
#### Let's imagine the following DAG ####
dag1 = dagify(y ~ x1+x2+x3+x4+d,
d ~ x1+x3+x4,
exposure = "d",
outcome = "y")
ggdag(dag1) + theme_dag()
#### Simulate some data based on dag1 ####
x1=rnorm(2000,0,2)
x2=rnorm(2000,5,1)
x3=rbinom(2000,1,0.5)
x4=sample(c("a","b","c"),2000,replace = T) %>% factor()
#convert categorical variable x4 to a numeric vector based on effect on d and y,
#a and c increase Pr(d=1), b decreases Pr(d=1)
x4_treat=recode(x4,a=0.8,b=-1,c=0.1)
x4_out=recode(x4,a=-2,b=0.2,c=1.5)
#create our vector of treatment assignment probabilities and plug into pnorm
p=0.8*x1-0.5*x3+x4_treat
pr=pnorm(p,mean(p),sd(p))
#assign treatment
d=rbinom(2000,1,pr)
#simulate outcome - our true effect of d on y is?
y=3+1.2*d+0.5*x1-x2+2*x3+x4_out+rnorm(2000,0,1)
data1=data.frame(y,d,x1,x2,x3,x4)
#how many units are in the treatment group?
sum(d)
#calculate raw difference in means and check covariate balance
diff_in_means=mean(data1$y[data1$d==1])-mean(data1$y[data1$d==0])
diff_in_means
balance=data1 %>% group_by(d) %>%
summarise(x1=mean(x1),
x2=mean(x2),
x3=mean(x3),
x4_A=sum(x4=="a")/n(),
x4_B=sum(x4=="b")/n(),
x4_C=sum(x4=="c")/n())
kbl(balance) %>% kable_paper("hover")
# ate will be biased, covariates will not be balanced
#calculate raw difference in means and check covariate balance
diff_in_means=mean(data1$y[data1$d==1])-mean(data1$y[data1$d==0])
diff_in_means
balance=data1 %>% group_by(d) %>%
summarise(x1=mean(x1),
x2=mean(x2),
x3=mean(x3),
x4_A=sum(x4=="a")/n(),
x4_B=sum(x4=="b")/n(),
x4_C=sum(x4=="c")/n())
kbl(balance) %>% kable_paper("hover")
#### Let's estimate our propensity scores using a logit model ####
ps1=glm(d~x1+x3+x4,data = data1, family = binomial(link = "logit"))
tidy(ps1)
#get predicted probabilities from this logit object
data1$pscore=predict(ps1,type = "response")
#let's see how the propensity scores of our treatment and control groups vary
data1$Group=factor(d)
ggplot(data = data1,aes(x=pscore,fill=Group))+
geom_density(alpha=0.3) + xlab("Propensity Score") +
scale_fill_discrete(labels=c("Control","Treatment"))
#create inverse propensity score weights for treatment and control
data1 = data1 %>%
mutate(wt=ifelse(d==1,1/pscore,1/(1-pscore)))
#now run the ipw models, have we estimated the effect of d on y?
weighted_diff_in_means=mean(data1$wt*data1$y*data1$d)-
mean(data1$wt*data1$y*(1-data1$d))
weighted_diff_in_means
#bootstrap error
boot_dim=c()
for (i in 1:1000) {
#resample observations
boot_data=data1[sample(1:nrow(data1),nrow(data1),replace = T),1:6]
#get propensity scores
boot_ps=glm(d~x1+x3+x4,data = boot_data,
family = binomial(link = "logit"))
boot_data$pscore=predict(boot_ps,type="response")
#weights
boot_data = boot_data %>%
mutate(wt=ifelse(d==1,1/pscore,1/(1-pscore)))
#calculate and sotre difference in means estimate
boot_dim[i]=mean(boot_data$wt*boot_data$y*boot_data$d)-
mean(boot_data$wt*boot_data$y*(1-boot_data$d))
}
#plot our bootstrap estimates
plot(density(boot_dim))
abline(v=1.2,lty=2,col="blue")
#get standard error estimate and calculate confidence interval
boot_se=sd(boot_dim)
weighted_ci=c(weighted_diff_in_means-1.96*boot_se,
weighted_diff_in_means+1.96*boot_se)
weighted_ci
#lastly check for balance
weighted_balance=data1 %>% group_by(d) %>%
summarise(x1=weighted.mean(x1,wt),
x2=weighted.mean(x2,wt),
x3=weighted.mean(x3,wt),
x4_A=sum(ifelse(x4=="a",1,0)*wt)/(sum(wt)),
x4_B=sum(ifelse(x4=="b",1,0)*wt)/sum(wt),
x4_C=sum(ifelse(x4=="c",1,0)*wt)/sum(wt))
kbl(weighted_balance) %>% kable_paper("hover")
rm(list=ls())
### here is the experimental sample ###
read_data <- function(df)
{
full_path <- paste("https://raw.github.com/scunning1975/mixtape/master/",
df, sep = "")
df <- read_dta(full_path)
return(df)
}
nsw_dw <- read_data("nsw_mixtape.dta")
#get mean of our treatment group
mean1 <- nsw_dw %>%
filter(treat == 1) %>%
pull(re78) %>%
mean()
#get mean of control
mean0 <- nsw_dw %>%
filter(treat == 0) %>%
pull(re78) %>%
mean()
#calculate the ate
ate <- mean1-mean0
ate
### now let's replace the experimental control with the general US population ###
#remove our experimental control group from the dataset
nsw_dw = nsw_dw %>%
filter(treat == 1)
# how many units are assigned to treatment?
nrow(nsw_dw)
#add in CPS data
read_data <- function(df)
{
full_path <- paste("https://raw.github.com/scunning1975/mixtape/master/",
df, sep = "")
df <- read_dta(full_path)
return(df)
}
nsw_dw_cpscontrol <- read_data("cps_mixtape.dta") %>%
bind_rows(nsw_dw) %>%
mutate(agesq = age^2,
agecube = age^3,
educsq = educ*educ,
u74 = case_when(re74 == 0 ~ 1, TRUE ~ 0),
u75 = case_when(re75 == 0 ~ 1, TRUE ~ 0),
interaction1 = educ*re74,
re74sq = re74^2,
re75sq = re75^2,
interaction2 = u74*hisp)
# how many control units are there?
nrow(nsw_dw_cpscontrol)-sum(nsw_dw_cpscontrol$treat)
#let's calculate the raw difference in means now
#get mean of our treatment group
cps_mean1 <- nsw_dw_cpscontrol %>%
filter(treat == 1) %>%
pull(re78) %>%
mean()
#get mean of control
cps_mean0 <- nsw_dw_cpscontrol %>%
filter(treat == 0) %>%
pull(re78) %>%
mean()
#calculate the ate
cps_ate <- cps_mean1-cps_mean0
cps_ate
# estimating the propensity scores
logit_nsw <- glm(treat ~ age + agesq + agecube + educ + educsq +
marr + nodegree + black + hisp + re74 + re75 + u74 +
u75 + interaction1, family = binomial(link = "logit"),
data = nsw_dw_cpscontrol)
nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>%
mutate(pscore = logit_nsw$fitted.values)
# mean pscore
pscore_control <- nsw_dw_cpscontrol %>%
filter(treat == 0) %>%
pull(pscore) %>%
mean()
pscore_treated <- nsw_dw_cpscontrol %>%
filter(treat == 1) %>%
pull(pscore) %>%
mean()
# histograms- pay attention to the y-axis
ggarrange(nsw_dw_cpscontrol %>%
filter(treat == 0) %>%
ggplot() + xlab("Propensity Score (Control)") +
geom_histogram(aes(x = pscore)),
nsw_dw_cpscontrol %>%
filter(treat == 1) %>%
ggplot() + xlab("Propensity Score (Treatment)") +
geom_histogram(aes(x = pscore)))
#### visualize this with a dag ####
nsw_dag = dagify(Y ~ D + X,
D ~ p,
p ~ X,
exposure = "D",outcome = "Y",
labels = c("Y"="earnings","D"="treatment",
"p"="p(X)","X"="X")) %>%
tidy_dagitty()
ggdag(nsw_dag,text = FALSE,use_labels = "label") + theme_dag_blank()
# if we condition on p(X), the effect of D on Y is identified
ggdag_adjust(nsw_dag, var = "p",text = FALSE,
use_labels = "label") + theme_dag()
# Manual ATE with non-normalized weights using all data
nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>%
mutate(y1 = treat * re78/pscore,
y0 = (1-treat) * re78/(1-pscore),
ht = y1 - y0)
#calculate the ATE
nsw_dw_cpscontrol %>%
pull(ht) %>%
mean()
# ^ weighting by propensity scores = dividing each outcome but its propensity score
# so units with scores very close to zero are having their value blown up
# and there are many, many more of these units ^ in control than in treatment
# trimming by propensity score
nsw_dw_cpscontrol_trim <- nsw_dw_cpscontrol %>%
select(-y1, -y0, -ht) %>%
filter(!(pscore >= 0.9)) %>%
filter(!(pscore <= 0.1))
nsw_dw_cpscontrol_trim <- nsw_dw_cpscontrol_trim %>%
mutate(y1 = treat * re78/pscore,
y0 = (1-treat) * re78/(1-pscore),
ht = y1 - y0)
#calculate the ATE
nsw_dw_cpscontrol_trim %>%
pull(ht) %>%
mean()
# import libraries
library(tidyverse)
library(haven)
library(readxl)
library(dplyr)
# read in excel data
GOTV <- read_excel("gotv_individual.xlsx")
# create num_voted column = sum of g2000, p2000, g2002, p2002 and p2004
GOTV$num_voted <- GOTV$g2000 + GOTV$p2000 + GOTV$g2002 + GOTV$p2002 + GOTV$p2004
# unique(GOTV$num_voted) yields 3 0 1 2 5 4 (all [0,5])
GOTV_clean <- subset(GOTV, select=c("hh_id", "hh_size", "num_voted",
"voted", "treatment"))
# unique(GOTV$treatment) yields "Civic Duty" "Hawthorne" "Control" "Self" "Neighbors"
GOTV_clean <- filter(GOTV_clean, treatment == "Control" | treatment=="Neighbors")
# unique(GOTV_clean$treatment) yields "Control" "Neighbors"
GOTV_hh <- GOTV_clean %>%
group_by(hh_id) %>%
summarise(across(c(hh_size, num_voted, voted, treatment), mean))
View(GOTV_clean)
View(GOTV_clean)
View(GOTV_clean)
#
# (hh_size = mean(hh_size, na.rm=TRUE),
#             num_voted = mean(num_voted, na.rm=TRUE),
#             voted = mean(voted, na.rm=TRUE),
#             treatment = mean(treatment, na.rm=TRUE))
#aggregate(GOTV_clean, list(GOTV_clean$hh_id), mean)
#ceiling(x): Rounds values up to nearest integer.
#datatype(GOTV_clean)
# import libraries
library(tidyverse)
library(haven)
library(readxl)
library(dplyr)
# read in excel data
GOTV <- read_excel("gotv_individual.xlsx")
# create num_voted column = sum of g2000, p2000, g2002, p2002 and p2004
GOTV$num_voted <- GOTV$g2000 + GOTV$p2000 + GOTV$g2002 + GOTV$p2002 + GOTV$p2004
# unique(GOTV$num_voted) yields 3 0 1 2 5 4 (all [0,5])
GOTV_clean <- subset(GOTV, select=c("hh_id", "hh_size", "num_voted",
"voted", "treatment"))
# unique(GOTV$treatment) yields "Civic Duty" "Hawthorne" "Control" "Self" "Neighbors"
GOTV_clean <- filter(GOTV_clean, treatment == "Control" | treatment=="Neighbors")
# unique(GOTV_clean$treatment) yields "Control" "Neighbors"
# GOTV_hh <- GOTV_clean %>%
#   group_by(hh_id) %>%
#   summarise(across(c(hh_size, num_voted, voted, treatment), mean))
#
# (hh_size = mean(hh_size, na.rm=TRUE),
#             num_voted = mean(num_voted, na.rm=TRUE),
#             voted = mean(voted, na.rm=TRUE),
#             treatment = mean(treatment, na.rm=TRUE))
#aggregate(GOTV_clean, list(GOTV_clean$hh_id), mean)
#ceiling(x): Rounds values up to nearest integer.
#datatype(GOTV_clean)
GOTV_hh <- GOTV_clean %>%
group_by(hh_id) %>%
summarise(hh_size = mean(hh_size))
View(GOTV_hh)
GOTV_hh <- GOTV_clean %>%
group_by(hh_id) %>%
summarise(num_coted = mean(num_voted))
GOTV_hh <- GOTV_clean %>%
group_by(hh_id) %>%
summarise(hh_size = mean(hh_size),
num_voted = mean(num_voted))
GOTV_hh <- GOTV_clean %>%
group_by(hh_id) %>%
summarise(hh_size = mean(hh_size),
num_voted = mean(num_voted),
voted = mean(voted))
GOTV_hh <- GOTV_clean %>%
group_by(hh_id) %>%
summarise(treatment = mean(treatment))
View(GOTV_clean)
View(GOTV_clean)
View(GOTV_clean)
# import libraries
library(tidyverse)
library(haven)
library(readxl)
library(dplyr)
# read in excel data
GOTV <- read_excel("gotv_individual.xlsx")
# create num_voted column = sum of g2000, p2000, g2002, p2002 and p2004
GOTV$num_voted <- GOTV$g2000 + GOTV$p2000 + GOTV$g2002 + GOTV$p2002 + GOTV$p2004
# unique(GOTV$num_voted) yields 3 0 1 2 5 4 (all [0,5])
GOTV_clean <- subset(GOTV, select=c("hh_id", "hh_size", "num_voted",
"voted", "treatment"))
# unique(GOTV$treatment) yields "Civic Duty" "Hawthorne" "Control" "Self" "Neighbors"
GOTV_clean <- filter(GOTV_clean, treatment == "Control" | treatment=="Neighbors")
# unique(GOTV_clean$treatment) yields "Control" "Neighbors"
unique(GOTV_clean)
# GOTV_hh <- GOTV_clean %>%
#   group_by(hh_id) %>%
#   summarise(hh_size = mean(hh_size),
#     num_voted = mean(num_voted),
#     voted = mean(voted),
#     treatment = mean(treatment))
# GOTV_hh <- GOTV_clean %>%
#   group_by(hh_id) %>%
#   summarise(treatment = mean(treatment))
#
#   summarise(across(c(hh_size, num_voted, voted, treatment), mean))
#
# (hh_size = mean(hh_size, na.rm=TRUE),
#             num_voted = mean(num_voted, na.rm=TRUE),
#             voted = mean(voted, na.rm=TRUE),
#             treatment = mean(treatment, na.rm=TRUE))
#aggregate(GOTV_clean, list(GOTV_clean$hh_id), mean)
#ceiling(x): Rounds values up to nearest integer.
#datatype(GOTV_clean)
unique(GOTV_clean$treatment)
trimws(GOTV_clean$treatment)
GOTV_clean$treatment <- trimws(GOTV_clean$treatment)
GOTV_hh <- GOTV_clean %>%
group_by(hh_id) %>%
summarise(treatment = mean(treatment))
